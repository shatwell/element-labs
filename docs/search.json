[
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html",
    "href": "qmd/05-distributions-fruits-tidyverse.html",
    "title": "Distribution and confidence intervals of Clementine fruits",
    "section": "",
    "text": "The example aims to demonstrate estimation and interpretation of confidence intervals. At the end, the two samples are compared with respect to variance and mean values.\nThe experimental hypotheses was, that weight and size of two samples of Clementine fruits differ. The result is to be visualized with bar charts or box plots. We use only the weight as an example, analysis of the other statistical parameters is left as an optional exercise.\nWe can now derive the following statistical hypotheses about the variance:\n\n\\(H_0\\): The variance of both samples is the same.\n\\(H_a\\): The samples have different variance.\n\nand about the mean:\n\n\\(H_0\\): The mean of both samples is the same.\n\\(H_a\\): The mean values of the samples are different.",
    "crumbs": [
      "Labs",
      "Distribution and confidence intervals of Clementine fruits"
    ]
  },
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html#prepare-and-inspect-data",
    "href": "qmd/05-distributions-fruits-tidyverse.html#prepare-and-inspect-data",
    "title": "Distribution and confidence intervals of Clementine fruits",
    "section": "3.1 Prepare and inspect data",
    "text": "3.1 Prepare and inspect data\n\nDownload the data set fruits-2023-hse.csv and use one of RStudio’s “Import Dataset” wizards.\nA better alternative is to use read.csv().\n\n\n#  ... do it\n\n\nplot everything, just for testing:\n\n\nplot(fruits)\n\n\nsplit table for box1 and box2:\n\n\nbox1 &lt;- subset(fruits, brand == \"box1\")\nbox2 &lt;- subset(fruits, brand == \"box2\")\n\n\ncompare weight of both groups:\n\n\nboxplot(box1$weight, box2$weight, names=c(\"box1\", \"box2\"))\n\nNote: It is also possible to use boxplot with the model formula syntax. This is the preferred way, because it does not require to split the data set beforehand:\n\nboxplot(weight ~ brand, data = fruits)",
    "crumbs": [
      "Labs",
      "Distribution and confidence intervals of Clementine fruits"
    ]
  },
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html#check-distribution",
    "href": "qmd/05-distributions-fruits-tidyverse.html#check-distribution",
    "title": "Distribution and confidence intervals of Clementine fruits",
    "section": "3.2 Check distribution",
    "text": "3.2 Check distribution\nWe can check the shape of distribution graphically. If mean values of the samples differ much, it has to be done separately for each sample.\n\n# use `hist`, `qqnorm`, `qqline`\n# ...",
    "crumbs": [
      "Labs",
      "Distribution and confidence intervals of Clementine fruits"
    ]
  },
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html#sample-statistics",
    "href": "qmd/05-distributions-fruits-tidyverse.html#sample-statistics",
    "title": "Distribution and confidence intervals of Clementine fruits",
    "section": "3.3 Sample statistics",
    "text": "3.3 Sample statistics\nIf we assume normal distribution of the data, we can estimate an approximate prediction interval from the sample parameters, i.e. in which size range we find 95% of the weights within one group.\nWe first calculate mean, sd, N and se for “box1” data set:\n\nbox1.mean &lt;- mean(box1$weight)\nbox1.sd   &lt;- sd(box1$weight)\nbox1.N    &lt;- length(box1$weight)\nbox1.se   &lt;- box1.sd/sqrt(box1.N)\n\nThen we estimate the two-sided 95% prediction interval for the sample, assuming normal distribution:\n\nbox1.95 &lt;- box1.mean + c(-1.96, 1.96) * box1.sd\nbox1.95\n\nInstead of using 1.96, we could also use the quantile function of the normal distribution instead, e.g. qnorm(0.975)for the upper interval or qnorm(c(0.025, 0.975)) for the lower and upper.\nIf the data set is large enough, we can compare the prediction interval from above with the empirical quantiles, i.e. take it directly from the data. Here we do not assume a normal or any other distribution.\n\nquantile(box1$weight, p = c(0.025, 0.975))\n\nNow we plot the data and indicate the 95% interval:\n\nplot(box1$weight)\nabline(h = box1.95, col=\"red\")\n\n… and the same as histogram:\n\nhist(box1$weight)\nabline(v = box1.95, col=\"red\")\nrug(box1$weight, col=\"blue\")",
    "crumbs": [
      "Labs",
      "Distribution and confidence intervals of Clementine fruits"
    ]
  },
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html#confidence-interval-of-the-mean",
    "href": "qmd/05-distributions-fruits-tidyverse.html#confidence-interval-of-the-mean",
    "title": "Distribution and confidence intervals of Clementine fruits",
    "section": "3.4 Confidence interval of the mean",
    "text": "3.4 Confidence interval of the mean\nThe confidence interval of the mean tells us how precise a mean value was estimated from data. If the sample size is “large enough”, the distribution of the raw data does not necessarily need to be normal distributed, because then mean values tend to approximate a normal distribution due to the central limit theorem.\n\n3.4.1 Confidence interval of the mean for the “box1” data\n\nCalculate the confidence interval of the mean value of the “box1” data set,\nuse +/- 1.96 or (better) the quantile of the t-distribution:\n\n\nbox1.ci &lt;- box1.mean + qt(p = c(0.025, 0.975), df = box1.N-1) * box1.se\n\nNow indicate the confidence interval of the mean in the histogram.\n\nabline(v = box1.ci, col=\"red\")\n\n\n\n3.4.2 Confidence interval for the mean of the “box2” data\nWe could now in principle do the same as above for the “box2” sample, but this would be rather cumbersome and boring. A more efficient method from package dplyr is shown below.",
    "crumbs": [
      "Labs",
      "Distribution and confidence intervals of Clementine fruits"
    ]
  },
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html#calculation-of-summary-statistics-with-dplyr",
    "href": "qmd/05-distributions-fruits-tidyverse.html#calculation-of-summary-statistics-with-dplyr",
    "title": "Distribution and confidence intervals of Clementine fruits",
    "section": "5.1 Calculation of summary statistics with dplyr",
    "text": "5.1 Calculation of summary statistics with dplyr\nSummarizing can be done with two functions, group_by that adds grouping information to a data frame and summarize to calculate summary statistics. In the following, we use the full data set with 4 groups.\n\nlibrary(\"dplyr\")\nfruits &lt;- read.csv(\"fruits-2023-hse.csv\")\n\nstats &lt;-\n  fruits |&gt;\n    group_by(brand) |&gt;\n    summarize(mean = mean(weight), sd=sd(weight), N=length(weight), se=sd/sqrt(N),\n              lwr = mean + qt(p = 0.025, df = N-1) * se,\n              upr = mean + qt(p = 0.975, df = N-1) * se\n             )\n\nstats",
    "crumbs": [
      "Labs",
      "Distribution and confidence intervals of Clementine fruits"
    ]
  },
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html#barchart-and-errorbars-with-ggplot2",
    "href": "qmd/05-distributions-fruits-tidyverse.html#barchart-and-errorbars-with-ggplot2",
    "title": "Distribution and confidence intervals of Clementine fruits",
    "section": "5.2 Barchart and errorbars with ggplot2",
    "text": "5.2 Barchart and errorbars with ggplot2\nWe can then use the table of summary statistics directly for a bar chart.\n\nlibrary(\"ggplot2\")\nstats |&gt;\n  ggplot(aes(x=brand, y=mean, min=lwr, max=upr))  +\n    geom_col() + geom_errorbar()",
    "crumbs": [
      "Labs",
      "Distribution and confidence intervals of Clementine fruits"
    ]
  },
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html#additional-tasks",
    "href": "qmd/05-distributions-fruits-tidyverse.html#additional-tasks",
    "title": "Distribution and confidence intervals of Clementine fruits",
    "section": "5.3 Additional tasks",
    "text": "5.3 Additional tasks\nRepeat the analysis with other properties of the fruits, e.g. width and height. Create box plots, analyse distribution, create bar charts.",
    "crumbs": [
      "Labs",
      "Distribution and confidence intervals of Clementine fruits"
    ]
  },
  {
    "objectID": "qmd/12-timeseries-trends.html",
    "href": "qmd/12-timeseries-trends.html",
    "title": "An introductory time series example",
    "section": "",
    "text": "The main scientific question of the following examples is the existence of a trend. However, most trend tests assume stationarity of the residuals, so the concept of stationarity is first introduced by means of two artificial data sets. Here we introduce the following concepts:\n\ntrend stationarity and difference stationarity\nautocorrelation and partial autocorrelation\ntest for stationarity\ntest for a monotonic trend\n\nThe general procedure should then be applied to two real data sets as an exercise. Please keep in mind that the main objective here is trend analysis. The concepts of stationarity and autocorrelation and the related tests are only used as pre-tests to check if simple trend tests are possible. Please note also the importance of the effect size, e.g. the temperature increase pear year.\nThe book of (Kleiber2008?) contains an excellent explanation of the methods described here and is strongly recommended for further reading.",
    "crumbs": [
      "Labs",
      "An introductory time series example"
    ]
  },
  {
    "objectID": "qmd/12-timeseries-trends.html#data-set",
    "href": "qmd/12-timeseries-trends.html#data-set",
    "title": "An introductory time series example",
    "section": "2.1 Data set",
    "text": "2.1 Data set\nThe data set “timeseries.txt” contains artificial data with specifically designed properties, similar to the TSP and DSP series in the tutorial https://tpetzoldt.github.io/elements/. The data can be entered to R either with read.table or with the import assistant, then we convert it to time series objects (ts), to make their analysis easier.\n\ndat &lt;- read.csv(\"timeseries.csv\", header = TRUE)\nTSP &lt;- ts(dat$TSP)\nDSP &lt;- ts(dat$DSP)\n\nIt is always a good idea to plot the data first.\n\npar(mfrow=c(1,2))\nplot(TSP)\nplot(DSP)",
    "crumbs": [
      "Labs",
      "An introductory time series example"
    ]
  },
  {
    "objectID": "qmd/12-timeseries-trends.html#autocorrelation-and-partial-autocorrelation",
    "href": "qmd/12-timeseries-trends.html#autocorrelation-and-partial-autocorrelation",
    "title": "An introductory time series example",
    "section": "2.2 Autocorrelation and partial autocorrelation",
    "text": "2.2 Autocorrelation and partial autocorrelation\nFirst, plot the autocorrelation (acf) and partial autocorrelation (pacf) of the DSP series:\n\npar(mfrow=c(1,2))\nacf(DSP)\npacf(DSP)\n\n\n\n\n\n\n\n\n… and interpret the results.\nThen plot the acf for both series, together with the autocorrelation of the differenced and residual time series:\n\npar(mfrow=c(2,3))\nacf(TSP)\nacf(diff(TSP))\nacf(residuals(lm(TSP~time(TSP))))\nacf(DSP)\nacf(diff(DSP))\nacf(residuals(lm(DSP~time(DSP))))\n\nHere, diff is used for differencing the time series i.e. to compute differences between consecutive values, while lm fits a linear regression from which residuals extracts the residuals.\nThe autocorrelation function acf can be used to identify specific patterns. A series is considered as approximately stationary, if all autocorrelations (except for \\(lag=0\\)) are “almost non-significant”.\nHint: Deconstruct the parenthetisized statements like acf(residuals(lm(TSP~time(TSP)))) into 3 separate lines to understand better what they do. Plot the data and the trend.",
    "crumbs": [
      "Labs",
      "An introductory time series example"
    ]
  },
  {
    "objectID": "qmd/12-timeseries-trends.html#stationarity-test",
    "href": "qmd/12-timeseries-trends.html#stationarity-test",
    "title": "An introductory time series example",
    "section": "2.3 Stationarity test",
    "text": "2.3 Stationarity test\nThe Kwiatkowski-Phillips-Schmidt-Shin test checks directly for stationarity, where \\(H_0\\) may be either level stationarity or trend stationarity. Don’t get confused:\n\nlevel stationary is just the same as stationary, the additional “level” just makes it clearer.\nin contrast, trend stationary is essentially non-stationary, but can easily be made stationary by subtracting a trend, because the residuals are stationary.\nthe warning message of the KPSS test is normal and not an “error”, its just an information that the true p-value is either smaller or greater than the printed value.\n\n\nlibrary(\"tseries\")\nkpss.test(TSP, null=\"Level\") # instationary\nkpss.test(TSP, null=\"Trend\") # stationary after trend removal\nkpss.test(DSP, null=\"Level\") # instationary\nkpss.test(DSP, null=\"Trend\") # still instationary\nkpss.test(diff(DSP), null=\"Level\")",
    "crumbs": [
      "Labs",
      "An introductory time series example"
    ]
  },
  {
    "objectID": "qmd/12-timeseries-trends.html#mann-kendall-test-for-trends",
    "href": "qmd/12-timeseries-trends.html#mann-kendall-test-for-trends",
    "title": "An introductory time series example",
    "section": "2.4 Mann-Kendall test for trends",
    "text": "2.4 Mann-Kendall test for trends\nThis is now finally the main test.\n\nlibrary(\"Kendall\")\nMannKendall(TSP) # correct only for trend stationary time series\nMannKendall(DSP) # wrong, because time series was difference stationary",
    "crumbs": [
      "Labs",
      "An introductory time series example"
    ]
  },
  {
    "objectID": "qmd/12-timeseries-trends.html#trend-of-air-temperature",
    "href": "qmd/12-timeseries-trends.html#trend-of-air-temperature",
    "title": "An introductory time series example",
    "section": "3.1 Trend of air temperature",
    "text": "3.1 Trend of air temperature\nThe plot seems to show an increasing trend, especially since 1980.\n\nplot(Tair)\n\nTest of stationarity\nTest for stationarity for the original and the trend adjusted series graphically with acf and quantitatively with the KPSS test:\n\nkpss.test(Tair)\n\nWe see that \\(p &lt; 0.01\\) so it is not “level stationary”!\nBut if we allow for a trend:\n\nkpss.test(Tair, null=\"Trend\")\n\n… we get \\(p &gt; 0.05\\) i.e. it is trend stationary (stationary after trend removal). Therefore, a trend test is possible.\nTrend test\nWe use the Mann-Kendall test dirst, that tests for monotonous trends:\n\nMannKendall(Tair)\n\nNow we fit a linear model to find out how much the temperature increased per day during this time.\n\nm &lt;- lm(Tair ~ time(Tair))\nsummary(m)\nplot(Tair)\nabline(m, col=\"red\")\n\nNow test the assumptions. Firstly test that the residuals have no autocorrelation:\n\nacf(residuals(m))\n\nOptional Task: use additional diagnostics, e.g. plot residuals versus fitted or qqnorm(residuals(m)) to test for normal distribution. Write the results down and evaluate what they can tell us.\nQuestions:\n\nIs the trend significant?\nwhich of the used tests is the best to test for a trend?\nwhat does “monotonous” mean?\nWhat is the advantage of fitting a linear model?\nWhat is the purpose of checking autocorrelation of the residuals with acf?",
    "crumbs": [
      "Labs",
      "An introductory time series example"
    ]
  },
  {
    "objectID": "qmd/12-timeseries-trends.html#stationarity-and-trend-of-water-temperature",
    "href": "qmd/12-timeseries-trends.html#stationarity-and-trend-of-water-temperature",
    "title": "An introductory time series example",
    "section": "3.2 Stationarity and trend of water temperature",
    "text": "3.2 Stationarity and trend of water temperature\nNow repeat the same for the water temperature data, interpret the results and write a short report. Read about limnology of stratified lakes in temperate climate zones and discuss reasons why the trend of water temperature is weaker or stronger than air temperature.\nScientific Questions\n\nWas there a significant trend in water and air temperature?\nHow much Kelvin (degrees centigrade) did the water temperature increase on average during this time?\nWas the trend of water weaker or stronger than for air temperature? Which lake-physical processes are responsible for this effect?",
    "crumbs": [
      "Labs",
      "An introductory time series example"
    ]
  },
  {
    "objectID": "qmd/10-nonlinear-regression.html",
    "href": "qmd/10-nonlinear-regression.html",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "The following examples demonstrate how to perform non-linear regression in R. This is quite different from linear regression, not only because the regression functions show a curve, but also due to the applied numerical techniques. While in the linear case, the coefficients of the regression line can be calculated directly (analytically) by solving a linear system of equations, iterative numerical optimization needs to be used instead.\nThis means that the coefficients are approximated step by step until convergence, beginning with start values specified by the user. There is no guarantee that an optimal solution can be found.\nThe following examples are intended as a starting point, the last example (logistic growth) is left as an exercise.\n\n\nThe first example shows an exponentially growing data set that is fitted by nonlinear least squares (nls). Here, the exponential function is given directly at the right hand side of the formula and the start values for the parameters are specified in pstart. An optional argument trace = TRUE is set to watch the progress of iteration.\nFunction predict can be used to create dense \\(x\\) and \\(y\\)-values (here: x1 and y1) for a smooth curve.\nThe statistical results of the regression are displayed with summary: parameters (\\(a\\), \\(b\\)) of the curve, standard errors and the correlation between \\(a\\) and \\(b\\). Note the optional correlation=TRUE argument. High correlations can indicate problems in model fitting, especially due to so called non-identifiablility of the parameter set.\nHere everything went through even with high correlation, because the data do not vary much around the exponential curve.\nFinally, the nonlinear coefficient of determination \\(R^2\\) can be calculated from the variances of the residuals and the original data:\n\nx &lt;- 1:10\ny &lt;- c(1.6, 1.8, 2.1, 2.8, 3.5, 4.1, 5.1, 5.8, 7.1, 9.0)\nplot(x, y)\n\npstart &lt;- c(a = 1, b = 1)\nmodel_fit &lt;- nls(y ~ a * exp(b * x), start = pstart, trace = TRUE)\nx1 &lt;- seq(1, 10, 0.1)\ny1 &lt;- predict(model_fit, data.frame(x = x1))\nlines(x1, y1, col = \"red\")\n\nsummary(model_fit, correlation = TRUE)\n\n## R squared\n1 - var(residuals(model_fit))/var(y)\n\n\n\n\nInstead of putting the regression model directly into nls it is also possible to use a user-defined function, that we call f here for example. This approach is especially useful for more complicated regression models:\n\nf &lt;- function(x, a, b) {a * exp(b * x)}\n\nplot(x, y)\n\npstart &lt;- c(a = 1, b = 1)\nmodel_fit &lt;- nls(y ~ f(x, a, b), start = pstart)\n\nx1 &lt;- seq(1, 10, 0.1)\ny1 &lt;- predict(model_fit, data.frame(x = x1))\nlines(x1, y1, col = \"red\")\nsummary(model_fit, correlation = TRUE)\n1 - var(residuals(model_fit))/var(y)",
    "crumbs": [
      "Labs",
      "Nonlinear Regression"
    ]
  },
  {
    "objectID": "qmd/10-nonlinear-regression.html#exponential-growth",
    "href": "qmd/10-nonlinear-regression.html#exponential-growth",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "The first example shows an exponentially growing data set that is fitted by nonlinear least squares (nls). Here, the exponential function is given directly at the right hand side of the formula and the start values for the parameters are specified in pstart. An optional argument trace = TRUE is set to watch the progress of iteration.\nFunction predict can be used to create dense \\(x\\) and \\(y\\)-values (here: x1 and y1) for a smooth curve.\nThe statistical results of the regression are displayed with summary: parameters (\\(a\\), \\(b\\)) of the curve, standard errors and the correlation between \\(a\\) and \\(b\\). Note the optional correlation=TRUE argument. High correlations can indicate problems in model fitting, especially due to so called non-identifiablility of the parameter set.\nHere everything went through even with high correlation, because the data do not vary much around the exponential curve.\nFinally, the nonlinear coefficient of determination \\(R^2\\) can be calculated from the variances of the residuals and the original data:\n\nx &lt;- 1:10\ny &lt;- c(1.6, 1.8, 2.1, 2.8, 3.5, 4.1, 5.1, 5.8, 7.1, 9.0)\nplot(x, y)\n\npstart &lt;- c(a = 1, b = 1)\nmodel_fit &lt;- nls(y ~ a * exp(b * x), start = pstart, trace = TRUE)\nx1 &lt;- seq(1, 10, 0.1)\ny1 &lt;- predict(model_fit, data.frame(x = x1))\nlines(x1, y1, col = \"red\")\n\nsummary(model_fit, correlation = TRUE)\n\n## R squared\n1 - var(residuals(model_fit))/var(y)",
    "crumbs": [
      "Labs",
      "Nonlinear Regression"
    ]
  },
  {
    "objectID": "qmd/10-nonlinear-regression.html#regression-model-as-user-defined-function",
    "href": "qmd/10-nonlinear-regression.html#regression-model-as-user-defined-function",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "Instead of putting the regression model directly into nls it is also possible to use a user-defined function, that we call f here for example. This approach is especially useful for more complicated regression models:\n\nf &lt;- function(x, a, b) {a * exp(b * x)}\n\nplot(x, y)\n\npstart &lt;- c(a = 1, b = 1)\nmodel_fit &lt;- nls(y ~ f(x, a, b), start = pstart)\n\nx1 &lt;- seq(1, 10, 0.1)\ny1 &lt;- predict(model_fit, data.frame(x = x1))\nlines(x1, y1, col = \"red\")\nsummary(model_fit, correlation = TRUE)\n1 - var(residuals(model_fit))/var(y)",
    "crumbs": [
      "Labs",
      "Nonlinear Regression"
    ]
  },
  {
    "objectID": "qmd/01-pivot-tables-with-libreoffice.html",
    "href": "qmd/01-pivot-tables-with-libreoffice.html",
    "title": "Discharge of River Elbe: Data Management with LibreOffice",
    "section": "",
    "text": "Planning and maintenance of waterways and rivers needs adequate measurements and data. However, raw time series can often be long and confusing, so a first step is aggregation and visualization.\nScientific aim: plot an average year and calculate monthly averages, minima and maxima of the discharge.",
    "crumbs": [
      "Labs",
      "Discharge of River Elbe: Data Management with LibreOffice"
    ]
  },
  {
    "objectID": "qmd/01-pivot-tables-with-libreoffice.html#download-the-data-set-and-inspect-the-data",
    "href": "qmd/01-pivot-tables-with-libreoffice.html#download-the-data-set-and-inspect-the-data",
    "title": "Discharge of River Elbe: Data Management with LibreOffice",
    "section": "3.1 Download the data set and inspect the data",
    "text": "3.1 Download the data set and inspect the data\n\nDownload the data set (elbe_data.ods) from the course home page and save it to a personal folder or your USB pendrive.\nOpen it with LibreOffice Calc. Excel has a similar functionality, but details differ.\nMake sure that you have set Calc to English language.\nInspect the data. You see that the date format has the form YYYY-MM-DD, that is the so-called “ISO 8601” date format, the international standard that makes data exchange between diferent software systems easier2.",
    "crumbs": [
      "Labs",
      "Discharge of River Elbe: Data Management with LibreOffice"
    ]
  },
  {
    "objectID": "qmd/01-pivot-tables-with-libreoffice.html#creation-of-categories",
    "href": "qmd/01-pivot-tables-with-libreoffice.html#creation-of-categories",
    "title": "Discharge of River Elbe: Data Management with LibreOffice",
    "section": "3.2 Creation of categories",
    "text": "3.2 Creation of categories\nIn the following, we intend to aggregate the discharge data according to certain criteria, e.g. year and month. This can be done with the “pivot table” tool, so before we can do this, we need to create additional columns with the categories.\n\n3.2.1 Date computations\nCreate the following categorical columns using formulas for date computation:\n\nyear:= YEAR(A2)\nmonth: = MONTH(A2)\nday: = DAY(A2)\nweekday: = WEEKDAY(A2)\ndoy: = A2 - DATE(YEAR(A2), 1, 1) + 1\n\nThe last formula computes thew “day of year” (doy), also called Julian day. Here DATE(YEAR(A2), 1, 1) creates a date for the first January of the respective year and then the difference (+1) between the respective day and the corresponding 1st January. The formula respects the different length of months automatically, including 29th February in leap years.\nThen fill the formulas down the column until the end of the data column.\nNote: The formulas above assume that you use LibreOffice with English user interface. If you use another language (e.g. German) or other program (e.g. Excel), then the keywords and delimiters (semi-colon instead of comma) of the formulas may be different and you have to look up for them in the function library.\n\n\n3.2.2 Pivot tables\nIn LibreOffice pivot tables are created like follows:\n\nSelect the data range for which the pivot table is to be created (including header!),\nIn the menu select Insert, Pivot Table\nSelect Source – Current selection – ok\nNow drag the appropriate items to the fields, e.g. “year” to the Column field, “month” to Row field and “discharge” to Data field.\nDouble click on “discharge” and change Function to “Average”.\nOK\n\nThat’s it, and you get the monthly average discharge values.\nTask: Repeat the same for individual years, and for all years to extract minimum and maximum discharge and find a way to show the results graphically.\n\n\n3.2.3 Average year and seasonality\nCreate a pivot table with “year” as Column fields, “doy” as Row fields and “Mean discharge” (i.e. Average) as Data fields.\nPlot all years as function of the day of year.\nThen create the some of following plots (you may need additional pivot tables):\n\na bar chart for annual discharge sums (y=annual discharge, x=year)\na bar chart with average discharge for the 12 months.\nXY (Scatter) chart for all years like example before, and in addition average discharge for all observed years as thick line. Note: here it may be necessary to copy the numbers only from the pivot table to a separate sheet (Paste special – numbers) to remove the pivot table automatism for the graphics.\nXY (Scatter) chart with confidence band. Calculate maximum, average and minimum per doy over all years 3 lines: y = discharge min / average / max, x = doy.\n\nNow interpret the results. What was 2002? Google for “Elbe river 2002”.\n\n\n3.2.4 Cumulative sum plot3\nCumulative sum plots of rainfall, discharge or temperature are useful for reservoir managers, or to classify years whether they are dry, wet, cold or warm.\nCreate a cumulative sum plot for each year by adding the discharge data (\\(Q\\)) as follows:\n\\[\\begin{align*}\nQ_{sum, 1} &= Q_1 \\\\\nQ_{sum, 2} &= Q_1 + Q_2\\\\\nQ_{sum, 3} &= Q_1 + Q_2 + Q_3\\\\\nQ_{sum, n} &= Q_1 + Q_2 + Q_3 + \\cdots + Q_n\\\\\n\\end{align*}\\]\nAnswer the following questions. Which year was:\n\nthe wettest,\nthe driest,\nhad a wet winter and a dry summer?\n\n\n\n3.2.5 Additional ideas\nThe following ideas are intended as a stimulus for own explorations of the data and creative work. Feel free just to play around with downloaded data or develop your own project.\n\nRepeat the analysis with an additional elbe.csv4 file with more years.\nDownload data from other measurement stations, e.g.\n\nfrom http://www.fgg-elbe.de/elbe-datenportal.html or\nU.S. Geological Survey http://waterdata.usgs.gov/\n\n\nand analyse the data.",
    "crumbs": [
      "Labs",
      "Discharge of River Elbe: Data Management with LibreOffice"
    ]
  },
  {
    "objectID": "qmd/01-pivot-tables-with-libreoffice.html#footnotes",
    "href": "qmd/01-pivot-tables-with-libreoffice.html#footnotes",
    "title": "Discharge of River Elbe: Data Management with LibreOffice",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nData Source: Federal Waterways and Shipping Administration (WSV), provided by the Federal Institute for Hydrology (BfG).↩︎\nFor details, see http://en.wikipedia.org/wiki/ISO_dates↩︎\noptional topic with higher difficulty↩︎\nhttps://github.com/tpetzoldt/datasets/blob/main/data/↩︎",
    "crumbs": [
      "Labs",
      "Discharge of River Elbe: Data Management with LibreOffice"
    ]
  },
  {
    "objectID": "00-index.html",
    "href": "00-index.html",
    "title": "Elements of applied statistics – lab exercises",
    "section": "",
    "text": "This website contains a collection of material for introductory statistics courses with R. The aim is to provide insight in fundamental principles and a broad overview and enable students to select and understand particular books and online material to dig in deeper in the diverse and fascinating field of statistics."
  },
  {
    "objectID": "00-index.html#introduction",
    "href": "00-index.html#introduction",
    "title": "Elements of applied statistics – lab exercises",
    "section": "",
    "text": "This website contains a collection of material for introductory statistics courses with R. The aim is to provide insight in fundamental principles and a broad overview and enable students to select and understand particular books and online material to dig in deeper in the diverse and fascinating field of statistics."
  },
  {
    "objectID": "00-index.html#status",
    "href": "00-index.html#status",
    "title": "Elements of applied statistics – lab exercises",
    "section": "Status",
    "text": "Status\nThe selection of material is work in progress. Additional material will appear until end of January 2025. Feedback and comments are welcome."
  },
  {
    "objectID": "00-index.html#author",
    "href": "00-index.html#author",
    "title": "Elements of applied statistics – lab exercises",
    "section": "Author",
    "text": "Author\nhttps://tu-dresden.de/Members/thomas.petzoldt\nhttps://github.com/tpetzoldt\n2024-10-21"
  },
  {
    "objectID": "qmd/02-discharge-elbe-project.html",
    "href": "qmd/02-discharge-elbe-project.html",
    "title": "Discharge of River Elbe: Homework Project",
    "section": "",
    "text": "Interpret the results of Sections 3.3 and 3.4. Discuss which of the plots best shows the shape of the distribution of discharge data. You can include up to 4 plots. Add a table with the statistical parameters (min, max, \\(\\bar{x}, s_x\\), median, geometric mean and quartiles).\nThen discuss all results (parameters and figures) in connection.",
    "crumbs": [
      "Projects",
      "Discharge of River Elbe: Homework Project"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe-project.html#communicate-in-your-team-with-other-teams-and-with-tutors",
    "href": "qmd/02-discharge-elbe-project.html#communicate-in-your-team-with-other-teams-and-with-tutors",
    "title": "Discharge of River Elbe: Homework Project",
    "section": "3.1 Communicate in your team, with other teams and with tutors",
    "text": "3.1 Communicate in your team, with other teams and with tutors\nCommunicate results and ideas in your team and also with other teams and with the tutors. It is a good idea to speak (or email) with your collegues first, then collect material and create a first version (this is often longer as the 5 pages).\nThen discuss and improve the result, select the most important parts and create the final version.\nCommunication can be done in person or in the matrix. The idea of the matrix is that all (= “the community”) can learn from each other. Private communication channels for teamwork are of course allowed, just find an agreement what fits your needs best. If you want, it is also possible to create separate communication channels in Opal or the Matrix for the groups.\nIf you ask in the forum or the matrix, please formulate specific questions and not only “this is my result, is this correct”. Good examples will hopefully develop during the work. Please feel free to post questions and contribute to the answers.",
    "crumbs": [
      "Projects",
      "Discharge of River Elbe: Homework Project"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe-project.html#technical-recommendations",
    "href": "qmd/02-discharge-elbe-project.html#technical-recommendations",
    "title": "Discharge of River Elbe: Homework Project",
    "section": "3.2 Technical recommendations",
    "text": "3.2 Technical recommendations\nThe report should read nicely. Space needed for figures, tables and explanatory text should be in good balance.\n\nThe report should have 5 pages at maximum. Quality instead of quantity! Distill the essential messages.\nFont size of the text should be 11 or 12 points.\nA line spacing of 1.2 lines is recommended to improve readability.\nFigures (lines, font size of annotations) must be well readable.\nLiterature must be properly cited. We encourage to use author-year style citations. Good examples can be found at the APA style web page.\n\nFinally, upload your result as PDF and (optionally) an R script to the File folder of your group. Submissions after the deadline cannot be considered.",
    "crumbs": [
      "Projects",
      "Discharge of River Elbe: Homework Project"
    ]
  },
  {
    "objectID": "qmd/10-nonlinear-regression-solution.html",
    "href": "qmd/10-nonlinear-regression-solution.html",
    "title": "Fit nonlinear model to plankton growth data",
    "section": "",
    "text": "The growth rate of a population is a direct measure of fitness. Therefore, determination of growth rates is common in many disciplines of natural and human sciences, business and engineering: ecology, pharmacology, wastewater treatment, and economic growth. The following example gives a brief introduction, how growth models can be fitted wit R.",
    "crumbs": [
      "Solutions",
      "Fit nonlinear model to plankton growth data"
    ]
  },
  {
    "objectID": "qmd/10-nonlinear-regression-solution.html#data-set",
    "href": "qmd/10-nonlinear-regression-solution.html#data-set",
    "title": "Fit nonlinear model to plankton growth data",
    "section": "2.1 Data set",
    "text": "2.1 Data set\nThe example data set was taken from a growth experiment in a batch culture with Microcystis aeruginosa, a cyanobacteria (blue green algae) species. Details of the experiment can be found in Jähnichen et al. (2001).\n\n## time (t)\nx &lt;- c(0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20)\n## Algae cell counts (per ml)\ny &lt;- c(0.88, 1.02, 1.43, 2.79, 4.61, 7.12,\n       6.47, 8.16, 7.28, 5.67, 6.91) * 1e6",
    "crumbs": [
      "Solutions",
      "Fit nonlinear model to plankton growth data"
    ]
  },
  {
    "objectID": "qmd/10-nonlinear-regression-solution.html#methods",
    "href": "qmd/10-nonlinear-regression-solution.html#methods",
    "title": "Fit nonlinear model to plankton growth data",
    "section": "2.2 Methods",
    "text": "2.2 Methods\nParametric models are fitted using nonlinear regression according to the method of least squares. Data analysis is performed using the R software of statistical computing and graphics (R Core Team, 2021) and the nls function from package stats. An additional analysis is performed with packages growthrates (Petzoldt, 2020) and FME (Soetaert & Petzoldt, 2010).\nTo get a suitable curve, we need a model that fits the data and that has identifiable parameters. In the following, we use the logistic growth model (Verhulst, 1838):\n\\[\nN = \\frac{K \\cdot N_0}{(N_0 + (K - N_0) \\cdot \\exp(-r \\cdot x))}\n\\]\nand the Baranyi-Roberts model (Baranyi & Roberts, 1994), explained later.",
    "crumbs": [
      "Solutions",
      "Fit nonlinear model to plankton growth data"
    ]
  },
  {
    "objectID": "qmd/10-nonlinear-regression-solution.html#nonlinear-regression-with-nls",
    "href": "qmd/10-nonlinear-regression-solution.html#nonlinear-regression-with-nls",
    "title": "Fit nonlinear model to plankton growth data",
    "section": "3.1 Nonlinear regression with “nls”",
    "text": "3.1 Nonlinear regression with “nls”\n\n3.1.1 Logistic Growth\nWe define now a used defined function for the logistic and this by plotting the function with the start values (blue line). Then we can use function nls (nonlinear least squares) to fit the model:\n\n## function definition\nf &lt;- function(x, r, K, N0) {K /(1 + (K/N0 - 1) * exp(-r *x))}\n\n## check of start values\nplot(x, yy, pch=16, xlab=\"time (days)\", ylab=\"algae (Mio cells)\")\nlines(x, f(x, r=r, K=max(yy), N0=yy[1]), col=\"blue\")\n\n## nonlinear regression\npstart &lt;- c(r=r, K=max(yy), N0=yy[1])\nfit_logistic   &lt;- nls(yy ~ f(x, r, K, N0), start = pstart, trace=FALSE)\n\nx1 &lt;- seq(0, 25, length = 100)\nlines(x1, predict(fit_logistic, data.frame(x = x1)), col = \"red\")\nlegend(\"topleft\",\n       legend = c(\"data\", \"start parameters\", \"fitted parameters\"),\n       col = c(\"black\", \"blue\", \"red\"),\n       lty = c(0, 1, 1),\n       pch = c(16, NA, NA))\n\n\n\n\n\n\n\nsummary(fit_logistic)\n\n\nFormula: yy ~ f(x, r, K, N0)\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \nr    0.5682     0.1686   3.371  0.00978 ** \nK    7.0725     0.4033  17.535 1.14e-07 ***\nN0   0.1757     0.1861   0.944  0.37271    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8118 on 8 degrees of freedom\n\nNumber of iterations to convergence: 14 \nAchieved convergence tolerance: 4.018e-06\n\n(Rsquared &lt;- 1 - var(residuals(fit_logistic))/var(yy))\n\n[1] 0.931732\n\n\nWe see that the fit converged and the red line approximates the data, but we can also see that the model fit is far below the data at the beginning. This will be improved in the next section.\n\n\n3.1.2 Baranyi-Roberts model\nThe logistic function assumes, that growth starts exponentially from the beginning and then approaches more and more saturation. In reality, organisms need often some time to adapt to new conditions, and we can observe a delay at the beginnig. This delay is called lag-phase. Several models exist to describe such behavior, where the Baranyi-Roberts model (Baranyi & Roberts, 1994) is one of the most commonly used. Its parameters are similar to the logistic function with one additional parameter \\(h_0\\) for the lag. Following its mathematical equation (not shown here), we can implement it a suser-defined function in R:\n\nbaranyi &lt;- function(x, r, K, N0, h0) {\n  A &lt;- x + 1/r * log(exp(-r * x) + exp(-h0) - exp(-r * x - h0))\n  y &lt;- exp(log(N0) + r * A - log(1 + (exp(r * A) - 1)/exp(log(K) - log(N0))))\n  y\n}\n\nIf we assume a lag time \\(h_0 = 2\\), we can try to fit it and compare it with the logistic model\n\npstart &lt;- c(r=0.5, K=7, N0=1, h0=2)\nfit_baranyi   &lt;- nls(yy ~ baranyi(x, r, K, N0, h0), start = pstart, trace=FALSE)\n\nplot(x, yy, pch=16, xlab=\"time (days)\", ylab=\"algae (Mio cells)\")\nlines(x1, predict(fit_logistic, data.frame(x = x1)), col = \"red\")\nlines(x1, predict(fit_baranyi, data.frame(x = x1)), col = \"forestgreen\", lwd=2)\n\nlegend(\"topleft\",\n       legend = c(\"data\", \"logistic model\", \"Baranyi-Roberts model\"),\n       col = c(\"black\", \"red\", \"forestgreen\"),\n       lty = c(0, 1, 1),\n       pch = c(16, NA, NA))\n\n\n\n\n\n\n\n\nIt is obvious, that it fits much better.",
    "crumbs": [
      "Solutions",
      "Fit nonlinear model to plankton growth data"
    ]
  },
  {
    "objectID": "qmd/10-nonlinear-regression-solution.html#growth-curve-fitting-with-r-package-growthrates",
    "href": "qmd/10-nonlinear-regression-solution.html#growth-curve-fitting-with-r-package-growthrates",
    "title": "Fit nonlinear model to plankton growth data",
    "section": "3.2 Growth curve fitting with R package “growthrates”",
    "text": "3.2 Growth curve fitting with R package “growthrates”\nAs growth curves are of fundamental importance in science and engineering, several R packages exist for this problem. Here we show one of these packages growthrates (Petzoldt, 2020). Details can be found in the package documentation.\n\n3.2.1 Maximum growth rate as steepest increase in log scale\nThe package contains a method “easy linear” to find the steepest linear increase. It is a fully automatic method employing linear regression and a search routine. Details of the algorithm are found in Hall et al. (2014).\nThe following shows the phase of steepest increase, the exponential phase, identified by linear regression using the data points with the steepest increase:\n\nlibrary(\"growthrates\")\npar(mfrow=c(1, 2))\nfit_easy &lt;- fit_easylinear(x, yy)\nplot(fit_easy, main=\"linear scale\")\nplot(fit_easy, log=\"y\", main=\"log scale\")\n\n\n\n\n\n\n\ncoef(fit_easy)\n\n       y0     y0_lm     mumax       lag \n0.8800000 0.5838576 0.2528382 1.6226381 \n\n\n\n\n3.2.2 Logistic growth\nNow we can take the start parameters from above and function fit_growthmodel using the grow_logistic function, that is pre-defined in the package. We can also use a specific plot function from the package\n\npstart &lt;- c(mumax=r, K=max(yy), y0=yy[1])\nfit_logistic2 &lt;- fit_growthmodel(grow_logistic, p=pstart, time=x, y=yy)\nplot(fit_logistic2)\n\n\n\n\n\n\n\n\n\n\n3.2.3 Baranyi-Roberts model\nWe see again that the model fits not very well at the beginning because of the lag phase. Therefore, we empoy again an extended model e.g. the Baranyi model.\nA start value for the lag phase parameter \\(h_0\\) can be approximated from the “easylinear” method:\n\ncoef(fit_easy)\n\n       y0     y0_lm     mumax       lag \n0.8800000 0.5838576 0.2528382 1.6226381 \n\nh0 &lt;- 0.25 * 1.66\n\npstart &lt;- c(mumax=0.5, K=max(yy), y0=yy[1], h0=h0)\nfit_baranyi2 &lt;- fit_growthmodel(grow_baranyi, p=pstart, time=x, y=yy)\nsummary(fit_baranyi2)\n\n\nParameters:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nmumax   0.8477     0.3681   2.303   0.0547 .  \nK       6.9969     0.3499  19.999 1.96e-07 ***\ny0      0.9851     0.5250   1.876   0.1027    \nh0      4.1220     3.0894   1.334   0.2239    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7583 on 7 degrees of freedom\n\nParameter correlation:\n        mumax       K      y0      h0\nmumax  1.0000 -0.3607  0.4600  0.9635\nK     -0.3607  1.0000 -0.1030 -0.2959\ny0     0.4600 -0.1030  1.0000  0.6477\nh0     0.9635 -0.2959  0.6477  1.0000\n\n\nThe summary shows the parameter estimates, their standard error and a significance level. However, we should not take the significance stars too seriously here. If we would, for example, omit the “nonsignificant” parameters y0 and h0, or set it to zero, the models would not work anymore. We see that some parameters correlate, especially h0 and y0. This can, in principle, indicate identification problems, but this dod not happen here, fortunatly.\nFinally, we plot the results in both, linear and log scale:\n\npar(mfrow=c(1, 2))\nplot(fit_logistic2, ylim=c(0, 10), las=1)\nlines(fit_baranyi2, col=\"magenta\")\n\npoints(x, yy, pch=16, col=\"red\")\n\n## log scale\nplot(fit_logistic2, log=\"y\", ylim=c(0.2, 10), las=1)\npoints(x, yy, pch=16, col=\"red\")\nlines(fit_baranyi2, col=\"magenta\")\nlegend(\"bottomright\",\n       legend = c(\"data\", \"logistic model\", \"Baranyi model\"),\n       col = c(\"red\", \"blue\", \"magenta\"),\n       lty = c(0, 1, 1),\n       pch = c(16, NA, NA))",
    "crumbs": [
      "Solutions",
      "Fit nonlinear model to plankton growth data"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html",
    "href": "qmd/02-discharge-elbe.html",
    "title": "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "",
    "text": "The following practical example demonstrates how data in “long format” can be analysed with R. It builds up on a previous exercise about date and time computation and pivot tables with LibreOfficce.\n\n\nThe example assumes that recent versions of R and RStudio are installed, together with some add-on packages dplyr, tidyr, readr, lubridate and ggplot2. The packages should already be available in the computer pool of the university, otherwise install it over the “Packages” pane in RStudio or from the command line:\nIf all packages are installed, we need to load it to the active session with\n\nlibrary(readr)     # modernized functions to read rectangular data like csv\nlibrary(dplyr)     # the most essential tidyverse packages\nlibrary(tidyr)     # contains for example pivot tables\nlibrary(lubridate) # a tidyverse package for dates\nlibrary(ggplot2)   # high level plotting with the grammar of graphics\n\nThe examples were tested with R versions 4.2.1 – 4.3.2.\n\n\n\nThe data set consists of daily measurements for discharge of the Elbe River in Dresden (daily discharge sum in \\(\\mathrm{m^3 s^{-1}}\\)). The data were kindly provided by the German Federal Institute for Hydrology (BfG)1.\nPlease read the information file elbe_info.txt about data source and copyright before downloading the data file “data.csv”. The data set ist then available in the course folder or from https://github.com/tpetzoldt/datasets/blob/main/data/.\n\n\n\nWe first learn how to import data to R, then we will do date and time conversion and create some plots. After that we learn how to aggregate, analyse and reformat the data set. A final outlook gives an impression how to use pipelines and high level plotting with the ggplot package.",
    "crumbs": [
      "Labs",
      "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#software-prerequisites",
    "href": "qmd/02-discharge-elbe.html#software-prerequisites",
    "title": "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "",
    "text": "The example assumes that recent versions of R and RStudio are installed, together with some add-on packages dplyr, tidyr, readr, lubridate and ggplot2. The packages should already be available in the computer pool of the university, otherwise install it over the “Packages” pane in RStudio or from the command line:\nIf all packages are installed, we need to load it to the active session with\n\nlibrary(readr)     # modernized functions to read rectangular data like csv\nlibrary(dplyr)     # the most essential tidyverse packages\nlibrary(tidyr)     # contains for example pivot tables\nlibrary(lubridate) # a tidyverse package for dates\nlibrary(ggplot2)   # high level plotting with the grammar of graphics\n\nThe examples were tested with R versions 4.2.1 – 4.3.2.",
    "crumbs": [
      "Labs",
      "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#the-data-set",
    "href": "qmd/02-discharge-elbe.html#the-data-set",
    "title": "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "",
    "text": "The data set consists of daily measurements for discharge of the Elbe River in Dresden (daily discharge sum in \\(\\mathrm{m^3 s^{-1}}\\)). The data were kindly provided by the German Federal Institute for Hydrology (BfG)1.\nPlease read the information file elbe_info.txt about data source and copyright before downloading the data file “data.csv”. The data set ist then available in the course folder or from https://github.com/tpetzoldt/datasets/blob/main/data/.",
    "crumbs": [
      "Labs",
      "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#overview",
    "href": "qmd/02-discharge-elbe.html#overview",
    "title": "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "",
    "text": "We first learn how to import data to R, then we will do date and time conversion and create some plots. After that we learn how to aggregate, analyse and reformat the data set. A final outlook gives an impression how to use pipelines and high level plotting with the ggplot package.",
    "crumbs": [
      "Labs",
      "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#import-of-spreadsheet-and-text-files",
    "href": "qmd/02-discharge-elbe.html#import-of-spreadsheet-and-text-files",
    "title": "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "2.1 Import of spreadsheet and text files",
    "text": "2.1 Import of spreadsheet and text files\nR can access spreadsheet tables and data bases directly using packages like readxl for Excel files. It can also read LibreOffice files and data bases.\nHere we want to make it simple and just read the data from a universal exchange format (.txt or .csv) that can be shared between all systems. In our example, we use a csv-file (comma separated values), where the first row is the table header of unique variable names. The variable names must start with a letter and should not contain special characters, spaces etc. Additional meta information (e.g. source of data) and measurement shold be documented separately, for example in a separate file README.txt.\nThe example file elbe.csv contains daily discharge of the Elbe River in \\(\\mathrm{m^3 s^{-1}}\\) from gauging station Dresden, river km 55.6 from the Federal Waterways and Shipping Administration (WSV) and where provided by the Federal Institute for Hydrology (BfG).\nThe third column “validated” indicates whether the values were finally approved by WSV and BfG. Data from the 19th century are particularly uncertain. Please consult the file elbe_info.txt for details.\n\n\n\nRStudio import assistant",
    "crumbs": [
      "Labs",
      "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#input-method-1-use-the-import-dataset-wizard-of-rstudio",
    "href": "qmd/02-discharge-elbe.html#input-method-1-use-the-import-dataset-wizard-of-rstudio",
    "title": "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "2.2 Input method 1: Use the import dataset wizard of RStudio",
    "text": "2.2 Input method 1: Use the import dataset wizard of RStudio\n\nFirst, download the file elbe.csv and store it to your working directory.\nNow Open RStudio and Select: File – Import Dataset – From Text (readr).\nOpen the file and you will see the import dataset assistant. Select the correct settings for your file and choose an appropriate name (e.g. elbe) for the data frame in R.",
    "crumbs": [
      "Labs",
      "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#input-method-2-read-data-directly-from-r",
    "href": "qmd/02-discharge-elbe.html#input-method-2-read-data-directly-from-r",
    "title": "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "2.3 Input method 2: Read data directly from R",
    "text": "2.3 Input method 2: Read data directly from R\n\nNavigate to the data file with the “files pane” (bottom right in Rstudio by default),\nIf you cannot find the file easily, use the dots (…) of the file pane.\nSelect: More – Set as Working Directory.\nRun the following commands in R:\n\n\nlibrary(\"readr\")\nelbe &lt;- read_csv(\"elbe.csv\")\n\nThis works if the data format is a true csv (comma separated values) file with English decimal dot “.” for the numbers and “,” for the column separator. If the file format is different, we may use read.table, a more flexible function that allows to specify the column separator decimal.\nNote: for the exercise, one of the above methods is sufficient, either the import wizard or read_csv. The command line method is advantageous if a file is read several times or if several files need to be imported.",
    "crumbs": [
      "Labs",
      "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#date-and-time-conversion",
    "href": "qmd/02-discharge-elbe.html#date-and-time-conversion",
    "title": "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "3.1 Date and time conversion",
    "text": "3.1 Date and time conversion\nIn the following we extend the elbe data frame by adding information about the day, month, year and day of year. Here function mutate adds additional columns, or modifies existing if the column names exist.\nNote also that the day of year function in the date and time package lubridate is named yday. Details about date and time conversion can be found in a cheatsheet available from https://raw.githubusercontent.com/rstudio/cheatsheets/main/lubridate.pdf\n\nelbe &lt;- mutate(elbe,\n               date  = as.Date(date), # may be redundant if read_csv was used\n               day   = day(date), \n               month = month(date), \n               year  = year(date), \n               doy   = yday(date))\n\nNow, have a look at the “Global Environment” pane and inspect the data structure of the elbe data frame.",
    "crumbs": [
      "Labs",
      "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#basic-plotting-with-rs-base-plot",
    "href": "qmd/02-discharge-elbe.html#basic-plotting-with-rs-base-plot",
    "title": "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "3.2 Basic plotting with R’s base plot",
    "text": "3.2 Basic plotting with R’s base plot\nThe full time series can be plotted using the date as argument for the x-axis and discharge for the y-axis. The $ sign indicates from which column of the elbe-table data are taken. The \"l\" indicates line plots.\n\nplot(elbe$date, elbe$discharge, type=\"l\")\n\nThe same can be done with a so-called formula syntax. Here y and x are given in opposite order, separated with a ~ (tilde sign). It can be read as “y as a function of x”. The formula syntax allows to specify the data as a separate argument.\n\nplot(discharge ~ date, data=elbe, type=\"l\")\n\n\n\n\n\n\n\n\nThe formula syntax has additional benefits, for example a subset argument:\n\nplot(discharge ~ doy, data=elbe, subset = year==2002, col=\"blue\", type=\"l\")\nlines(discharge ~ doy, data=elbe, subset = year==2003, col=\"red\")\n\nExercise: Plot 4 years with 4 different colors, 2 wet and 2 dry years.",
    "crumbs": [
      "Labs",
      "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#histograms",
    "href": "qmd/02-discharge-elbe.html#histograms",
    "title": "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "3.3 Histograms",
    "text": "3.3 Histograms\nHistograms show the distribution of the data. Compare the shape of following three:\n\nHistogram with untransformed data\nHistogram with log-transformed data\nHistogram with log-transformed data, where a certain baseflow is subtracted before taking the log.\n\n\nhist(elbe$discharge)\nhist(log(elbe$discharge))\nhist(log(elbe$discharge - 0.9 * min(elbe$discharge)))\n\nExercises:\n\nDiscuss, which of the three histograms best describe discharge distribution.\nRepeat the plot with smaller classes, e.g. hist(elbe$discharge, breaks=50).",
    "crumbs": [
      "Labs",
      "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#boxplots",
    "href": "qmd/02-discharge-elbe.html#boxplots",
    "title": "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "3.4 Boxplots",
    "text": "3.4 Boxplots\nBoxplots are a very compact way to visualize the distribution of data:\n\nboxplot(elbe$discharge)\n\nExercise: Create boxplots for:\n\nlog-transformed discharge,\nlog-transformed value of discharge - baseflow.\nInterpret the results: What do the “middle line”, the box, the whiskers and the extreme values tell us?\nDiscuss the “outliers”: how many, at which side and if they are really “outliers”.",
    "crumbs": [
      "Labs",
      "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#cumulative-sums",
    "href": "qmd/02-discharge-elbe.html#cumulative-sums",
    "title": "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "3.5 Cumulative sums",
    "text": "3.5 Cumulative sums\nAnnual cumulative sum plots are a hydrological standard tool used by reservoir managers. We can use the R function cumsum, that by successive cumulation converts a sequence of:\n\\(x_1, x_2, x_3, x_4, \\dots\\) into\n\\((x_1), (x_1+x_2), (x_1+x_2+x_3), (x_1+x_2+x_3+x_4), \\dots\\)\nIf we just use cumsum for daily discharge (in \\(\\mathrm{m^3 s^{-1}}\\)) and multiply it with the number of seconds per day / 1e6, we get a cumulative sum in Mio \\(\\mathrm{m^3}\\) over all years:\n\nelbe$cum &lt;- cumsum(elbe$discharge) * 60*60*24 / 1e6\nplot(elbe$date, elbe$cum, type=\"l\", ylab=\"Mio m^3\")\n\nHowever, cumulation is more commonly done per year, i.e. each year should start with the discharge from a given start day. In the following, let’s start with 1st of January, experts may consider to modify the code, to use the German hydrological year.\n\none_year     &lt;- subset(elbe, year == 2000)\none_year$cum &lt;- cumsum(one_year$discharge) * 60*60*24 / 1e6\nplot(one_year$date, one_year$cum, type=\"l\", ylab=\"Mio m^3\")\n\nHere, a steep increase shows a wet period, a flat curve indicates a dry period.",
    "crumbs": [
      "Labs",
      "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#summaries-and-cross-tabulation",
    "href": "qmd/02-discharge-elbe.html#summaries-and-cross-tabulation",
    "title": "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "4.1 Summaries and cross-tabulation",
    "text": "4.1 Summaries and cross-tabulation\nHere we use the tidyverse method summarize, after grouping with group_by. It is, compared to the classical aggregate-function i R more powerful and much easier to use:\n\n## calculate annual mean, minimum, maximum\nelbe_grouped &lt;- group_by(elbe, year)\n\ntotals &lt;- summarize(elbe_grouped, \n            mean = mean(discharge), \n            min = min(discharge), \n            max = max(discharge))\ntotals\n\nExercise: Use the above method to compute annual total discharge sums and monthly average discharge values.",
    "crumbs": [
      "Labs",
      "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#a-standard-pivot-table",
    "href": "qmd/02-discharge-elbe.html#a-standard-pivot-table",
    "title": "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "4.2 A standard pivot table",
    "text": "4.2 A standard pivot table\nTidyverse provides also tools for the conversion of data base tables (long data format) into cross-tables (wide data format) and vice versa. This functionality changed several times in the last years, so you may see functions like melt and cast or gather and spread doing more or less the same, but with different syntax. The most recent development suggests the two functions pivot_wider and pivot_longer for this purpose.\nIts first argument is a data base table, the other arguments define the structure of the desired crosstable.\nHere id_cols is the name of a column in a long table that will become the rows, names_from indicates where the names of the columns are taken from and values_from the column with the values for the cross table. If more than one value is possible for a row x column combination, an optional values_fn can be given.\n\nelbe_wide &lt;-  pivot_wider(elbe, \n                id_cols = doy, \n                names_from = year, \n                values_from = discharge, \n                #values_fn = mean\n              )\nelbe_wide\n\nExercise: Create a crosstable for monthly max. discharge over all years.",
    "crumbs": [
      "Labs",
      "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#back-conversion-of-a-crosstable-into-a-data-base-table",
    "href": "qmd/02-discharge-elbe.html#back-conversion-of-a-crosstable-into-a-data-base-table",
    "title": "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "4.3 Back-conversion of a crosstable into a data base table",
    "text": "4.3 Back-conversion of a crosstable into a data base table\nThe inverse case is also possible, e.g. the conversion of a cross table into a data base table. It can be done with the function pivot_longer. The column of the id.vars variable(s) will become identifier(s) downwards.\n\npivot_longer(elbe_wide, names_to=\"year\", cols=as.character(1989:2019))",
    "crumbs": [
      "Labs",
      "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#minimum-maximum-plot-with-summarize-and-ggplot2",
    "href": "qmd/02-discharge-elbe.html#minimum-maximum-plot-with-summarize-and-ggplot2",
    "title": "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "5.1 Minimum-Maximum plot with summarize and ggplot2",
    "text": "5.1 Minimum-Maximum plot with summarize and ggplot2\n\n## Read data\nelbe &lt;- read.csv(\"elbe.csv\")\n\n## do everything in one pipeline:\n##   doy calculation; grouping; min, max, mean; melt to long format; plotting\nelbe |&gt; \n  mutate(doy = yday(date)) |&gt;\n  group_by(doy) |&gt;\n  summarize(max = max(discharge), \n            mean = mean(discharge), \n            min = min(discharge)) |&gt;\n  pivot_longer(cols = c(\"min\", \"mean\", \"max\"), \n               names_to = \"statistic\", \n               values_to = \"discharge\") |&gt;\n  ggplot(aes(doy, discharge, color = statistic)) + geom_line()",
    "crumbs": [
      "Labs",
      "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#cumulative-sums-for-all-years",
    "href": "qmd/02-discharge-elbe.html#cumulative-sums-for-all-years",
    "title": "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "5.2 Cumulative sums for all years",
    "text": "5.2 Cumulative sums for all years\nCumulative sums are a standard tool used by hydrologists and reservoir managers. They allow to detect easily dry and wet years and periods.\nIf we just use cumsum, we get a cumulative sum over all years:\n\nelbe |&gt; \n  mutate(doy = yday(date), year = year(date)) |&gt;\n  filter(year %in% 2000:2010) |&gt;\n  group_by(year = factor(year)) |&gt;\n  mutate(cum_discharge = cumsum(discharge) * 60*60*24) |&gt;\n  ggplot(aes(doy, cum_discharge, color = year)) + geom_line()\n\n\n\n\n\n\n\n\nExercises:\n\nWhich year was the wettest, which one the driest year in total? Find a year with dry spring and wet summer. Use the outcommented filter to reduce the number of simultanaeous lines.\nModify the commands so that the hydrological year is shown. Note that the German hydrological year goes from 1st November to 31st October of the following year. Other countries have different regulations.",
    "crumbs": [
      "Labs",
      "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#footnotes",
    "href": "qmd/02-discharge-elbe.html#footnotes",
    "title": "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nData Source: Federal Waterways and Shipping Administration (WSV), provided by the Federal Institute for Hydrology (BfG).↩︎",
    "crumbs": [
      "Labs",
      "Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  }
]